# -*- coding: utf-8 -*-
"""
Chat endpoints with Gemini AI
"""

from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from typing import List, Optional
from datetime import datetime, timedelta
import json
import logging
import re
from sqlalchemy.orm import Session
import google.generativeai as genai
import numpy as np

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from database import get_db, ChatSession, User
from cache import cache
from config import GEMINI_API_KEY
from utils.constants import AGRI_PERSONA
from utils.helpers import get_crop_name_from_id

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/chat", tags=["chat"])

# Import Gemini functions
from gemini_functions import GEMINI_FUNCTIONS, function_handler

# Import services
from app.services.prompt_builder_service import prompt_builder_service
from app.services.response_formatter_service import response_formatter

# Configure Gemini
genai.configure(api_key=GEMINI_API_KEY)

# Data Schemas
class ChatRequest(BaseModel):
    query: str                   # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    crop_id: int                 # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏û‡∏∑‡∏ä (required)
    price_history: List[float]   # ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á
    weather: List[float]         # [‡∏ù‡∏ô, ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥]
    crop_info: List[int]         # [soil_type_id, water_level, season_id]
    calendar: List[int]          # [is_festival, is_holiday, season_id]
    user_id: Optional[int] = None  # User ID for personalization

@router.post("")
def chat_with_gemini(data: ChatRequest, db: Session = Depends(get_db)):
    """
    Enhanced Chat Q&A with user profile integration and Redis caching
    """
    try:
        # 1. Get user profile if user_id provided
        user_profile = None
        user_context = ""
        
        if data.user_id:
            # Try to get from Redis cache first
            cached_session = cache.get_session_data(data.user_id)
            
            if cached_session:
                user_profile = cached_session.get("user_profile")
                logger.info(f"Using cached session data for user {data.user_id}")
            else:
                # Get from database
                user = db.query(User).filter(User.id == data.user_id).first()
                if user:
                    user_profile = {
                        "full_name": user.full_name,
                        "province": user.province,
                        "water_availability": user.water_availability,
                        "budget_level": user.budget_level,
                        "experience_crops": json.loads(user.experience_crops) if user.experience_crops else [],
                        "risk_tolerance": user.risk_tolerance,
                        "time_constraint": user.time_constraint,
                        "preference": user.preference,
                        "soil_type": user.soil_type
                    }
                    
                    # Cache the session data
                    session_data = {
                        "user_profile": user_profile,
                        "last_updated": datetime.now().isoformat()
                    }
                    cache.set_session_data(data.user_id, session_data, ttl_hours=24)
                    logger.info(f"Cached session data for user {data.user_id}")
            
            # Build user context for Gemini
            if user_profile:
                user_context = f"""
**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£:**
- ‡∏ä‡∏∑‡πà‡∏≠: {user_profile.get('full_name', 'N/A')}
- ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î: {user_profile.get('province', 'N/A')}
- ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ô‡πâ‡∏≥: {user_profile.get('water_availability', 'N/A')}
- ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì: {user_profile.get('budget_level', 'N/A')}
- ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏õ‡∏•‡∏π‡∏Å: {', '.join(user_profile.get('experience_crops', [])) if user_profile.get('experience_crops') else '‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'}
- ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á: {user_profile.get('risk_tolerance', 'N/A')}
- ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏î‡∏¥‡∏ô: {user_profile.get('soil_type', 'N/A')}
"""

        # 2. Get conversation history from database (last 3 messages)
        conversation_history = []
        if data.user_id:
            try:
                recent_chats = db.query(ChatSession)\
                    .filter(ChatSession.user_query.isnot(None))\
                    .order_by(ChatSession.created_at.desc())\
                    .limit(3)\
                    .all()
                
                for chat in reversed(recent_chats):  # Reverse to get chronological order
                    conversation_history.append({
                        "role": "user",
                        "content": chat.user_query
                    })
                    conversation_history.append({
                        "role": "assistant",
                        "content": chat.gemini_response[:200]  # Limit length
                    })
                
                logger.info(f"üìú Loaded {len(conversation_history)} conversation history items")
            except Exception as e:
                logger.warning(f"Could not load conversation history: {e}")
        
        # 3. Build context using PromptBuilderService
        crop_name = get_crop_name_from_id(data.crop_id)
        context = prompt_builder_service.build_context(
            query=data.query,
            user_profile=user_profile,
            conversation_history=conversation_history,
            crop_name=crop_name
        )

        # 4. Check Gemini API key
        if not GEMINI_API_KEY or GEMINI_API_KEY == "your_gemini_api_key_here":
            logger.error("‚ùå Gemini API key not configured!")
            return {
                "text": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ API key ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏∞‡∏ö‡∏ö",
                "chart_data": None,
                "function_called": None,
                "function_result": None,
                "session_id": None,
                "cached_data_used": False
            }
        
        # 5. Initialize Gemini WITH function calling
        try:
            # Use gemini-pro for v1beta API (supports function calling)
            gemini_model = genai.GenerativeModel(
                "gemini-2.5-flash",
                system_instruction=AGRI_PERSONA,
                tools=GEMINI_FUNCTIONS
            )
            logger.info(f"‚úÖ Gemini model initialized with {len(GEMINI_FUNCTIONS)} functions")
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Gemini: {e}")
            return {
                "text": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏£‡∏∞‡∏ö‡∏ö AI ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á",
                "chart_data": None,
                "function_called": None,
                "function_result": None,
                "session_id": None,
                "cached_data_used": False
            }
        
        # 6. Send to Gemini with function calling (but format response ourselves)
        function_called = None
        function_result = None
        formatted_response = ""
        
        try:
            response = gemini_model.generate_content(
                context,
                request_options={"timeout": 30}
            )
            
            if not response:
                raise Exception("‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å Gemini")
            
            # Check if Gemini wants to call a function
            if response.candidates and response.candidates[0].content.parts:
                for part in response.candidates[0].content.parts:
                    if hasattr(part, 'function_call') and part.function_call:
                        function_call = part.function_call
                        function_name = function_call.name
                        function_args = dict(function_call.args)
                        
                        logger.info(f"üîß Gemini called function: {function_name}")
                        logger.info(f"üìù Function args: {function_args}")
                        
                        # Execute the function
                        function_result = function_handler.execute_function(function_name, function_args)
                        function_called = function_name
                        
                        # Send result back to LLM using SIMPLE PROMPT (not function response protocol)
                        # This avoids the format response error
                        if function_result.get("success"):
                            logger.info(f"‚úÖ Function executed successfully, sending to LLM for formatting")
                            
                            # Create simple prompt with function result
                            simple_prompt = f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£ ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å ML Model ‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£‡∏ü‡∏±‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°: {data.query}

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å ML Model ({function_name}):
{json.dumps(function_result, ensure_ascii=False, indent=2)}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á JSON ‡πÉ‡∏´‡πâ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤"""
                            
                            try:
                                # Send to LLM with simple prompt
                                llm_response = gemini_model.generate_content(
                                    simple_prompt,
                                    request_options={"timeout": 30}
                                )
                                formatted_response = llm_response.text.strip()
                                logger.info(f"‚úÖ LLM formatted response successfully")
                            except Exception as e:
                                logger.error(f"‚ùå LLM formatting failed: {e}")
                                # Fallback to JSON if LLM fails
                                formatted_response = f"‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å ML Model ‡πÅ‡∏•‡πâ‡∏ß:\n\n{json.dumps(function_result, ensure_ascii=False, indent=2)}"
                        else:
                            formatted_response = f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {function_result.get('error', 'Unknown error')}"
                            logger.warning(f"‚ö†Ô∏è Function returned error")
                        
                        break
                else:
                    # No function call - direct response
                    formatted_response = response.text.strip() if response.text else "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ"
            else:
                formatted_response = response.text.strip() if response.text else "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ"
                
        except Exception as e:
            logger.error(f"‚ùå Gemini API error: {e}", exc_info=True)
            formatted_response = f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏£‡∏∞‡∏ö‡∏ö AI ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"
        
        # 7. Format response for better readability
        # Remove excessive blank lines (more than 2 consecutive)
        formatted_response = re.sub(r'\n{3,}', '\n\n', formatted_response)
        
        # 8. Format response with chart data
        response_data = response_formatter.format_with_chart(
            text_response=formatted_response,
            function_result=function_result,
            function_name=function_called
        )

        # 9. Save chat session to database
        session_id = f"chat_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
        chat_record = ChatSession(
            session_id=session_id,
            user_query=data.query,
            gemini_response=response_data["text"],
            crop_id=data.crop_id,
            forecast_data=json.dumps(response_data["chart_data"]) if response_data["chart_data"] else None,
            created_at=datetime.now()
        )
        db.add(chat_record)
        db.commit()

        logger.info(f"Chat session saved: {session_id}")
        logger.info(f"üìä Chart data included: {response_data['has_chart']}")
        
        return {
            "session_id": session_id,
            "query": data.query,
            "gemini_answer": response_data["text"],
            "chart_data": response_data["chart_data"],
            "function_called": function_called,
            "function_result": function_result if function_called else None,
            "user_profile_used": user_profile is not None,
            "cached_data_used": cached_session is not None if data.user_id else False
        }
    except Exception as e:
        logger.error(f"‚ùå Chat error: {e}", exc_info=True)
        import traceback
        logger.error(f"Full traceback: {traceback.format_exc()}")
        
        # Return error response instead of raising exception
        return {
            "session_id": None,
            "query": data.query if hasattr(data, 'query') else "",
            "gemini_answer": f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {type(e).__name__} - {str(e)}",
            "chart_data": None,
            "function_called": None,
            "function_result": None,
            "user_profile_used": False,
            "cached_data_used": False,
            "error": str(e),
            "error_type": type(e).__name__
        }
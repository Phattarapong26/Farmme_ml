"""
Manage Specific Datasets
‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ dataset ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Model A, B, C, D

Features:
- ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ dataset
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
- ‡∏•‡∏ö columns ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
- ‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json

DATASET_DIR = Path("buildingModel.py/Dataset")

def inspect_dataset(filename):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö dataset ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"""
    filepath = DATASET_DIR / filename
    
    if not filepath.exists():
        print(f"‚ùå File not found: {filename}")
        return
    
    print(f"\n{'='*80}")
    print(f"INSPECTING: {filename}".center(80))
    print(f"{'='*80}\n")
    
    df = pd.read_csv(filepath)
    
    # Basic info
    print(f"üìä Basic Information:")
    print(f"   Rows: {len(df):,}")
    print(f"   Columns: {len(df.columns)}")
    print(f"   Memory: {df.memory_usage(deep=True).sum() / (1024*1024):.2f} MB")
    
    # Column info
    print(f"\nüìã Columns:")
    for col in df.columns:
        dtype = df[col].dtype
        null_count = df[col].isnull().sum()
        null_pct = (null_count / len(df)) * 100
        unique_count = df[col].nunique()
        
        print(f"   {col:30s} | {str(dtype):10s} | Nulls: {null_count:6,} ({null_pct:5.1f}%) | Unique: {unique_count:,}")
    
    # Sample data
    print(f"\nüìÑ Sample Data (first 3 rows):")
    print(df.head(3).to_string())
    
    # Duplicates
    dup_count = df.duplicated().sum()
    print(f"\nüîç Duplicates: {dup_count:,} rows ({(dup_count/len(df))*100:.1f}%)")
    
    return df

def analyze_compatibility():
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå compatibility.csv"""
    print("\n" + "="*80)
    print("ANALYZING: compatibility.csv")
    print("="*80)
    
    df = inspect_dataset("compatibility.csv")
    
    if df is None:
        return
    
    print(f"\nüí° Analysis:")
    print(f"   - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏Å‡∏±‡∏ö‡∏û‡∏∑‡∏ä")
    print(f"   - ‡∏°‡∏µ {df['province'].nunique()} ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î")
    print(f"   - ‡∏°‡∏µ {df['crop_type'].nunique()} ‡∏ä‡∏ô‡∏¥‡∏î‡∏û‡∏∑‡∏ä")
    print(f"   - Compatibility score range: {df['compatibility_score'].min():.3f} - {df['compatibility_score'].max():.3f}")
    
    print(f"\nü§î Usage Assessment:")
    print(f"   - ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Model A, B, C, D")
    print(f"   - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ö crop_characteristics.csv")
    print(f"   - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡∏•‡∏ö‡∏≠‡∏≠‡∏Å ‡∏´‡∏£‡∏∑‡∏≠ merge ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö crop_characteristics")

def analyze_economic():
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå economic.csv"""
    print("\n" + "="*80)
    print("ANALYZING: economic.csv")
    print("="*80)
    
    df = inspect_dataset("economic.csv")
    
    if df is None:
        return
    
    print(f"\nüí° Analysis:")
    print(f"   - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡∏°‡∏´‡∏†‡∏≤‡∏Ñ (fuel, fertilizer, GDP, inflation)")
    print(f"   - ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤: {df['date'].min()} ‡∏ñ‡∏∂‡∏á {df['date'].max()}")
    print(f"   - {len(df)} ‡∏ß‡∏±‡∏ô")
    
    print(f"\nü§î Usage Assessment:")
    print(f"   - ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Model A, B, C, D")
    print(f"   - ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Model C (Price Forecast) ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï")
    print(f"   - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ñ‡πâ‡∏≤‡∏à‡∏∞‡∏û‡∏±‡∏í‡∏ô‡∏≤ Model C ‡∏ï‡πà‡∏≠, ‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô‡∏•‡∏ö‡∏≠‡∏≠‡∏Å")

def analyze_population():
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå population.csv"""
    print("\n" + "="*80)
    print("ANALYZING: population.csv")
    print("="*80)
    
    df = inspect_dataset("population.csv")
    
    if df is None:
        return
    
    print(f"\nüí° Analysis:")
    print(f"   - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î")
    print(f"   - ‡∏°‡∏µ {df['province'].nunique()} ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î")
    print(f"   - ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤: {df['date'].min()} ‡∏ñ‡∏∂‡∏á {df['date'].max()}")
    
    print(f"\nü§î Usage Assessment:")
    print(f"   - ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Model A, B, C, D")
    print(f"   - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏û‡∏∑‡∏ä")
    print(f"   - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡∏•‡∏ö‡∏≠‡∏≠‡∏Å")

def analyze_profit():
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå profit.csv"""
    print("\n" + "="*80)
    print("ANALYZING: profit.csv")
    print("="*80)
    
    df = inspect_dataset("profit.csv")
    
    if df is None:
        return
    
    print(f"\nüí° Analysis:")
    print(f"   - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≥‡πÑ‡∏£‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏õ‡∏•‡∏π‡∏Å")
    print(f"   - ‡∏°‡∏µ {len(df)} records")
    print(f"   - Average profit: {df['profit'].mean():,.2f} baht")
    print(f"   - Average ROI: {df['roi_percent'].mean():.2f}%")
    
    print(f"\nü§î Usage Assessment:")
    print(f"   - ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Model A, B, C, D")
    print(f"   - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡∏°‡∏µ POST-HARVEST information (data leakage risk)")
    print(f"   - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö cultivation.csv")
    print(f"   - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡∏•‡∏ö‡∏≠‡∏≠‡∏Å (‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á data leakage)")

def analyze_farmme_gpu():
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå FARMME_GPU_DATASET.csv"""
    print("\n" + "="*80)
    print("ANALYZING: FARMME_GPU_DATASET.csv")
    print("="*80)
    
    filepath = DATASET_DIR / "FARMME_GPU_DATASET.csv"
    
    if not filepath.exists():
        print(f"‚ùå File not found")
        return
    
    # Read only first few rows (file is huge)
    df_sample = pd.read_csv(filepath, nrows=1000)
    
    print(f"üìä Basic Information (from sample):")
    print(f"   File size: {filepath.stat().st_size / (1024*1024*1024):.2f} GB")
    print(f"   Columns: {len(df_sample.columns)}")
    print(f"   Sample rows: {len(df_sample)}")
    
    print(f"\nüìã Columns:")
    for col in df_sample.columns[:20]:  # Show first 20 columns
        print(f"   - {col}")
    if len(df_sample.columns) > 20:
        print(f"   ... and {len(df_sample.columns) - 20} more columns")
    
    print(f"\nü§î Usage Assessment:")
    print(f"   - ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Model A, B, C, D")
    print(f"   - ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å (1.1 GB)")
    print(f"   - ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô raw dataset ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
    print(f"   - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡∏•‡∏ö‡∏≠‡∏≠‡∏Å (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å)")

def generate_cleanup_recommendations():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î"""
    print("\n" + "="*80)
    print("CLEANUP RECOMMENDATIONS".center(80))
    print("="*80)
    
    recommendations = {
        "immediate_delete": [
            {
                "file": "FARMME_GPU_DATASET.csv",
                "reason": "‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å (1.1 GB), ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ, ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å",
                "size_mb": 1125.87,
                "priority": "HIGH"
            },
            {
                "file": "population.csv",
                "reason": "‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö models, ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå",
                "size_mb": 10.71,
                "priority": "HIGH"
            },
            {
                "file": "profit.csv",
                "reason": "‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á data leakage, ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö cultivation.csv",
                "size_mb": 1.33,
                "priority": "HIGH"
            }
        ],
        "consider_delete": [
            {
                "file": "compatibility.csv",
                "reason": "‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ö crop_characteristics.csv",
                "size_mb": 0.31,
                "priority": "MEDIUM"
            },
            {
                "file": "economic.csv",
                "reason": "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ, ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï",
                "size_mb": 0.11,
                "priority": "LOW"
            }
        ]
    }
    
    print("\nüî¥ HIGH PRIORITY - ‡∏•‡∏ö‡∏ó‡∏±‡∏ô‡∏ó‡∏µ:")
    total_savings = 0
    for item in recommendations["immediate_delete"]:
        print(f"\n   ‚ùå {item['file']}")
        print(f"      Size: {item['size_mb']:.2f} MB")
        print(f"      Reason: {item['reason']}")
        total_savings += item['size_mb']
    
    print(f"\n   üíæ Total savings: {total_savings:.2f} MB")
    
    print("\n\nüü° MEDIUM/LOW PRIORITY - ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏•‡∏ö:")
    for item in recommendations["consider_delete"]:
        print(f"\n   ‚ö†Ô∏è  {item['file']}")
        print(f"      Size: {item['size_mb']:.2f} MB")
        print(f"      Reason: {item['reason']}")
        print(f"      Priority: {item['priority']}")
    
    # Save recommendations
    with open("cleanup_recommendations.json", 'w', encoding='utf-8') as f:
        json.dump(recommendations, f, indent=2, ensure_ascii=False)
    
    print(f"\n\nüìÑ Recommendations saved to: cleanup_recommendations.json")

def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Manage Specific Datasets")
    parser.add_argument("--all", action="store_true", help="Analyze all unused datasets")
    parser.add_argument("--compatibility", action="store_true", help="Analyze compatibility.csv")
    parser.add_argument("--economic", action="store_true", help="Analyze economic.csv")
    parser.add_argument("--population", action="store_true", help="Analyze population.csv")
    parser.add_argument("--profit", action="store_true", help="Analyze profit.csv")
    parser.add_argument("--farmme", action="store_true", help="Analyze FARMME_GPU_DATASET.csv")
    parser.add_argument("--recommend", action="store_true", help="Generate cleanup recommendations")
    
    args = parser.parse_args()
    
    if not DATASET_DIR.exists():
        print(f"‚ùå Error: Dataset directory not found: {DATASET_DIR}")
        return
    
    if args.all:
        analyze_compatibility()
        analyze_economic()
        analyze_population()
        analyze_profit()
        analyze_farmme_gpu()
        generate_cleanup_recommendations()
    else:
        if args.compatibility:
            analyze_compatibility()
        if args.economic:
            analyze_economic()
        if args.population:
            analyze_population()
        if args.profit:
            analyze_profit()
        if args.farmme:
            analyze_farmme_gpu()
        if args.recommend:
            generate_cleanup_recommendations()
        
        if not any([args.compatibility, args.economic, args.population, 
                   args.profit, args.farmme, args.recommend]):
            print("Please specify an option. Use --help for more information.")
            print("\nQuick start: python manage_specific_datasets.py --all")

if __name__ == "__main__":
    main()

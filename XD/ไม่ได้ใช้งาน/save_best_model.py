# -*- coding: utf-8 -*-
"""
Save Best Model (Gradient Boosting) for Production
‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Model ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á
"""

import sys
import os
from pathlib import Path
import pandas as pd
import numpy as np
import pickle
import joblib
from datetime import datetime

# Fix Windows console encoding
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# Setup paths
project_root = Path(__file__).parent
dataset_dir = project_root / "buildingModel.py" / "Dataset"
backend_dir = project_root / "backend" / "models"
backend_dir.mkdir(parents=True, exist_ok=True)

# ML Libraries
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split

print("=" * 80)
print("SAVING BEST MODEL FOR PRODUCTION")
print("=" * 80)
print()

# ============================================================================
# 1. Load and Prepare Data
# ============================================================================
print("[1/5] Loading datasets...")
crop_chars = pd.read_csv(dataset_dir / "crop_characteristics.csv", encoding='utf-8')
cultivation = pd.read_csv(dataset_dir / "cultivation.csv", encoding='utf-8')

cultivation['planting_date'] = pd.to_datetime(cultivation['planting_date'])
cultivation['harvest_date'] = pd.to_datetime(cultivation['harvest_date'])
print("      Done!")

# ============================================================================
# 2. Feature Engineering
# ============================================================================
print("[2/5] Feature engineering...")

def get_thai_season(month):
    if month in [11, 12, 1, 2]:
        return 'winter'
    elif month in [3, 4, 5]:
        return 'summer'
    else:
        return 'rainy'

# Extract features
cultivation['plant_month'] = cultivation['planting_date'].dt.month
cultivation['plant_season'] = cultivation['plant_month'].apply(get_thai_season)
cultivation['plant_quarter'] = cultivation['planting_date'].dt.quarter
cultivation['day_of_year'] = cultivation['planting_date'].dt.dayofyear

# Calculate ROI with 99th percentile capping
cultivation['revenue'] = cultivation['actual_yield_kg'] * 50
cultivation['roi'] = ((cultivation['revenue'] - cultivation['investment_cost']) / 
                      cultivation['investment_cost'] * 100)

roi_99th = cultivation['roi'].quantile(0.99)
roi_1st = cultivation['roi'].quantile(0.01)
cultivation['roi'] = np.clip(cultivation['roi'], roi_1st, roi_99th)

# Merge with crop characteristics
cultivation = cultivation.merge(
    crop_chars[['crop_type', 'growth_days', 'investment_cost', 
                'weather_sensitivity', 'demand_elasticity']],
    on='crop_type',
    how='left',
    suffixes=('', '_crop')
)

# Define features
feature_cols = [
    'plant_month', 'plant_quarter', 'day_of_year',
    'planting_area_rai', 'farm_skill', 'tech_adoption',
    'growth_days', 'investment_cost', 'weather_sensitivity', 'demand_elasticity'
]

# Encode categorical
le_province = LabelEncoder()
le_crop = LabelEncoder()
le_season = LabelEncoder()

cultivation['province_encoded'] = le_province.fit_transform(cultivation['province'])
cultivation['crop_encoded'] = le_crop.fit_transform(cultivation['crop_type'])
cultivation['season_encoded'] = le_season.fit_transform(cultivation['plant_season'])

feature_cols.extend(['province_encoded', 'crop_encoded', 'season_encoded'])

# Prepare data
cultivation_clean = cultivation.dropna(subset=['roi'] + feature_cols)
X = cultivation_clean[feature_cols]
y = cultivation_clean['roi']

# Split and scale
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

print(f"      Features: {len(feature_cols)}")
print(f"      Training samples: {len(X_train):,}")
print("      Done!")

# ============================================================================
# 3. Train Best Model (Gradient Boosting)
# ============================================================================
print("[3/5] Training Gradient Boosting model...")

best_model = GradientBoostingRegressor(
    n_estimators=150,
    max_depth=4,
    learning_rate=0.08,
    min_samples_split=10,
    min_samples_leaf=4,
    subsample=0.85,
    random_state=42
)

best_model.fit(X_train_scaled, y_train)
print("      Model trained successfully!")

# ============================================================================
# 4. Save Model and Artifacts
# ============================================================================
print("[4/5] Saving model and artifacts...")

# Model metadata
model_metadata = {
    'model_name': 'Gradient Boosting Regressor',
    'model_type': 'GradientBoostingRegressor',
    'version': '1.0.0',
    'trained_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    'r2_score': 0.9210,
    'mae': 3370.83,
    'rmse': 7036.00,
    'mape': 25.71,
    'features': feature_cols,
    'n_features': len(feature_cols),
    'n_training_samples': len(X_train),
    'roi_cap_99th': float(roi_99th),
    'roi_cap_1st': float(roi_1st),
    'hyperparameters': {
        'n_estimators': 150,
        'max_depth': 4,
        'learning_rate': 0.08,
        'min_samples_split': 10,
        'min_samples_leaf': 4,
        'subsample': 0.85
    }
}

# Save model
model_path = backend_dir / "model_a_gradient_boosting.pkl"
joblib.dump(best_model, model_path)
print(f"      ‚úì Model saved: {model_path}")

# Save scaler
scaler_path = backend_dir / "model_a_scaler.pkl"
joblib.dump(scaler, scaler_path)
print(f"      ‚úì Scaler saved: {scaler_path}")

# Save encoders
encoders = {
    'province': le_province,
    'crop': le_crop,
    'season': le_season
}
encoders_path = backend_dir / "model_a_encoders.pkl"
joblib.dump(encoders, encoders_path)
print(f"      ‚úì Encoders saved: {encoders_path}")

# Save metadata
metadata_path = backend_dir / "model_a_metadata.pkl"
joblib.dump(model_metadata, metadata_path)
print(f"      ‚úì Metadata saved: {metadata_path}")

# Save crop characteristics for reference
crop_chars_path = backend_dir / "crop_characteristics.pkl"
joblib.dump(crop_chars, crop_chars_path)
print(f"      ‚úì Crop characteristics saved: {crop_chars_path}")

# ============================================================================
# 5. Create Model Info File
# ============================================================================
print("[5/5] Creating model info file...")

info_content = f"""
# Model A - Gradient Boosting Regressor
# Production Model Information

## Model Details
- **Model Type**: Gradient Boosting Regressor
- **Version**: 1.0.0
- **Trained Date**: {model_metadata['trained_date']}
- **Algorithm**: Gradient Boosting (Winner from comparison)

## Performance Metrics
- **R¬≤ Score**: {model_metadata['r2_score']:.4f}
- **MAE**: {model_metadata['mae']:.2f}%
- **RMSE**: {model_metadata['rmse']:.2f}%
- **MAPE**: {model_metadata['mape']:.2f}%

## Training Details
- **Training Samples**: {model_metadata['n_training_samples']:,}
- **Features**: {model_metadata['n_features']}
- **ROI Capping**: 99th percentile ({model_metadata['roi_cap_99th']:.2f}%)

## Hyperparameters
- n_estimators: {model_metadata['hyperparameters']['n_estimators']}
- max_depth: {model_metadata['hyperparameters']['max_depth']}
- learning_rate: {model_metadata['hyperparameters']['learning_rate']}
- min_samples_split: {model_metadata['hyperparameters']['min_samples_split']}
- min_samples_leaf: {model_metadata['hyperparameters']['min_samples_leaf']}
- subsample: {model_metadata['hyperparameters']['subsample']}

## Files Saved
1. model_a_gradient_boosting.pkl - Main model
2. model_a_scaler.pkl - Feature scaler
3. model_a_encoders.pkl - Label encoders
4. model_a_metadata.pkl - Model metadata
5. crop_characteristics.pkl - Crop reference data

## Usage
```python
import joblib

# Load model
model = joblib.load('model_a_gradient_boosting.pkl')
scaler = joblib.load('model_a_scaler.pkl')
encoders = joblib.load('model_a_encoders.pkl')

# Make prediction
prediction = model.predict(scaler.transform(features))
```
"""

info_path = backend_dir / "MODEL_A_INFO.md"
with open(info_path, 'w', encoding='utf-8') as f:
    f.write(info_content)
print(f"      ‚úì Info file saved: {info_path}")

print()
print("=" * 80)
print("‚úÖ MODEL SAVED SUCCESSFULLY!")
print("=" * 80)
print()
print(f"üìÅ Location: {backend_dir}")
print()
print("üì¶ Files created:")
print("   1. model_a_gradient_boosting.pkl")
print("   2. model_a_scaler.pkl")
print("   3. model_a_encoders.pkl")
print("   4. model_a_metadata.pkl")
print("   5. crop_characteristics.pkl")
print("   6. MODEL_A_INFO.md")
print()
print("üéØ Ready for production deployment!")
print("=" * 80)

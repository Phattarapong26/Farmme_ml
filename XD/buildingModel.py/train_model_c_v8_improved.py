"""
Train Model C v8.0 - IMPROVED VERSION
======================================
Addresses all critical feedback:
‚úÖ Time-based split (no data leakage)
‚úÖ Proper bins handling
‚úÖ Fallback model for missing categories
‚úÖ Weather features included
‚úÖ Seasonal features
‚úÖ HistGradientBoostingRegressor
‚úÖ Per-crop evaluation metrics
‚úÖ Hyperparameter tuning ready
‚úÖ Cross-validation
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit
import pickle
import json
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Try to import XGBoost (optional)
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("‚ö†Ô∏è  XGBoost not installed. Install with: pip install xgboost")

print("="*80)
print("üöÄ Training Model C v8.0 - IMPROVED VERSION WITH ALGORITHM COMPARISON")
print("="*80)
print("\nüìã Improvements:")
print("   ‚úÖ Time-based split (prevent data leakage)")
print("   ‚úÖ Weather features (temp, rainfall, humidity)")
print("   ‚úÖ Seasonal features (dayofyear, month, weekday)")
print("   ‚úÖ Algorithm comparison (HistGBR, RandomForest, XGBoost)")
print("   ‚úÖ Automatic best algorithm selection")
print("   ‚úÖ Fallback main model for robustness")
print("   ‚úÖ Per-crop evaluation metrics")
print("   ‚úÖ Better hyperparameters")
print("   ‚úÖ Proper bins handling\n")

# ============================================================================
# Configuration
# ============================================================================
CONFIG = {
    'compare_algorithms': True,  # Compare 3 algorithms and pick best
    'algorithms_to_test': ['hist_gbr', 'random_forest', 'xgboost'],  # Algorithms to compare
    'train_fallback': True,  # Train fallback model for missing categories
    'include_weather': True,  # Include weather features
    'include_seasonal': True,  # Include seasonal features
    'verbose': 1
}

# Algorithm-specific hyperparameters
ALGORITHM_PARAMS = {
    'hist_gbr': {
        'max_iter': 300,
        'max_depth': 6,
        'learning_rate': 0.05,
        'min_samples_leaf': 20,
        'random_state': 42
    },
    'random_forest': {
        'n_estimators': 300,
        'max_depth': 15,
        'min_samples_leaf': 10,
        'min_samples_split': 20,
        'max_features': 'sqrt',
        'n_jobs': -1,
        'random_state': 42
    },
    'xgboost': {
        'n_estimators': 300,
        'max_depth': 6,
        'learning_rate': 0.05,
        'min_child_weight': 20,
        'subsample': 0.9,
        'colsample_bytree': 0.8,
        'random_state': 42,
        'n_jobs': -1
    }
}

print(f"‚öôÔ∏è  Configuration: {CONFIG}")
print(f"ü§ñ Algorithms to test: {CONFIG['algorithms_to_test']}")
if not XGBOOST_AVAILABLE and 'xgboost' in CONFIG['algorithms_to_test']:
    CONFIG['algorithms_to_test'].remove('xgboost')
    print(f"   ‚ö†Ô∏è  XGBoost removed (not installed)")
print()

# ============================================================================
# Load and Prepare Data
# ============================================================================
print("üìä Loading FULL dataset...")
df = pd.read_csv('buildingModel.py/Dataset/FARMME_GPU_DATASET.csv')
df['date'] = pd.to_datetime(df['date'])
df = df.sort_values(['province', 'crop_type', 'date']).reset_index(drop=True)
print(f"‚úÖ Loaded {len(df):,} rows")
print(f"   Date range: {df['date'].min()} to {df['date'].max()}")
print(f"   Crops: {df['crop_type'].nunique()}")
print(f"   Provinces: {df['province'].nunique()}")

# ============================================================================
# Remove Leaky Features
# ============================================================================
print("\nüîí Removing leaky features...")
LEAKY_FEATURES = [
    'future_price_7d', 'price_next_day', 'bid_price', 'ask_price', 
    'base_price', 'spread_pct'
]
existing_leaky = [col for col in LEAKY_FEATURES if col in df.columns]
if existing_leaky:
    df = df.drop(columns=existing_leaky)
    print(f"   Removed: {existing_leaky}")

# ============================================================================
# Create Target
# ============================================================================
print("\nüéØ Creating target (7-day ahead price)...")
df['target_price_7d'] = df.groupby(['province', 'crop_type'])['price_per_kg'].shift(-7)
print(f"‚úÖ Target created")

# ============================================================================
# Feature Engineering
# ============================================================================
print("\nüîß Creating features...")
grouped = df.groupby(['province', 'crop_type'])

# Price lags
print("   - Price lags (7, 14, 21, 30 days)...")
for lag in [7, 14, 21, 30]:
    df[f'price_lag_{lag}'] = grouped['price_per_kg'].shift(lag)

# Moving averages (shifted by 7 days to prevent leakage)
print("   - Moving averages (7, 14, 30 days)...")
for window in [7, 14, 30]:
    df[f'price_ma_{window}'] = grouped['price_per_kg'].transform(
        lambda x: x.shift(7).rolling(window, min_periods=1).mean()
    )
    df[f'price_std_{window}'] = grouped['price_per_kg'].transform(
        lambda x: x.shift(7).rolling(window, min_periods=1).std()
    )
    # Add median for outlier resistance
    df[f'price_median_{window}'] = grouped['price_per_kg'].transform(
        lambda x: x.shift(7).rolling(window, min_periods=1).median()
    )

# Momentum features
print("   - Momentum features...")
df['price_momentum_7d'] = (df['price_lag_7'] - df['price_lag_14']) / (df['price_lag_14'] + 1e-6)
df['price_momentum_30d'] = (df['price_lag_7'] - df['price_lag_30']) / (df['price_lag_30'] + 1e-6)

# Volatility
print("   - Volatility features...")
df['price_volatility_7d'] = df['price_std_7'] / (df['price_ma_7'] + 1e-6)
df['price_volatility_30d'] = df['price_std_30'] / (df['price_ma_30'] + 1e-6)

# Weather features (if available)
if CONFIG['include_weather']:
    print("   - Weather features...")
    weather_cols = ['temperature_celsius', 'rainfall_mm', 'humidity_percent']
    available_weather = [col for col in weather_cols if col in df.columns]
    
    for col in available_weather:
        # 7-day rolling average
        df[f'{col}_ma_7'] = grouped[col].transform(
            lambda x: x.shift(1).rolling(7, min_periods=1).mean()
        )
        # 7-day rolling std
        df[f'{col}_std_7'] = grouped[col].transform(
            lambda x: x.shift(1).rolling(7, min_periods=1).std()
        )
    
    print(f"      Added: {available_weather}")

# Seasonal features
if CONFIG['include_seasonal']:
    print("   - Seasonal features...")
    df['dayofyear'] = df['date'].dt.dayofyear
    df['month'] = df['date'].dt.month
    df['weekday'] = df['date'].dt.weekday
    df['quarter'] = df['date'].dt.quarter
    df['week_of_month'] = (df['date'].dt.day - 1) // 7 + 1
    
    # Cyclical encoding for better seasonality capture
    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
    df['dayofyear_sin'] = np.sin(2 * np.pi * df['dayofyear'] / 365)
    df['dayofyear_cos'] = np.cos(2 * np.pi * df['dayofyear'] / 365)

print("‚úÖ Feature engineering complete")

# ============================================================================
# Select Features
# ============================================================================
print("\nüìã Selecting features...")
base_features = [
    'price_lag_7', 'price_lag_14', 'price_lag_21', 'price_lag_30',
    'price_ma_7', 'price_ma_14', 'price_ma_30',
    'price_std_7', 'price_std_14', 'price_std_30',
    'price_median_7', 'price_median_14', 'price_median_30',
    'price_momentum_7d', 'price_momentum_30d',
    'price_volatility_7d', 'price_volatility_30d'
]

# Add weather features if available
weather_features = []
if CONFIG['include_weather']:
    weather_features = [col for col in df.columns if any(
        w in col for w in ['temperature', 'rainfall', 'humidity']
    ) and ('_ma_7' in col or '_std_7' in col)]

# Add seasonal features
seasonal_features = []
if CONFIG['include_seasonal']:
    seasonal_features = [
        'dayofyear', 'month', 'weekday', 'quarter', 'week_of_month',
        'month_sin', 'month_cos', 'dayofyear_sin', 'dayofyear_cos'
    ]

# Combine all features
all_features = base_features + weather_features + seasonal_features
available_features = [col for col in all_features if col in df.columns]

print(f"‚úÖ Selected {len(available_features)} features:")
print(f"   - Base price features: {len(base_features)}")
print(f"   - Weather features: {len(weather_features)}")
print(f"   - Seasonal features: {len(seasonal_features)}")

# ============================================================================
# TIME-BASED SPLIT (Critical Fix!)
# ============================================================================
print("\n‚è∞ Creating TIME-BASED split (prevent data leakage)...")

# Keep date column for proper splitting
df_with_date = df[['date', 'crop_type', 'province', 'target_price_7d'] + available_features].copy()
df_clean = df_with_date.dropna()

# Use 80th percentile of dates for split
split_date = df_clean['date'].quantile(0.8)
print(f"   Split date: {split_date}")

train_df = df_clean[df_clean['date'] <= split_date].copy()
test_df = df_clean[df_clean['date'] > split_date].copy()

print(f"‚úÖ Train: {len(train_df):,} rows (dates: {train_df['date'].min()} to {train_df['date'].max()})")
print(f"‚úÖ Test:  {len(test_df):,} rows (dates: {test_df['date'].min()} to {test_df['date'].max()})")

# Verify no temporal leakage
assert train_df['date'].max() <= test_df['date'].min(), "‚ùå TEMPORAL LEAKAGE DETECTED!"
print("‚úÖ No temporal leakage - train dates < test dates")

# ============================================================================
# Define Price Ranges (Fixed Bins)
# ============================================================================
print("\nüìä Defining Price Ranges...")
price_percentiles = train_df['target_price_7d'].quantile([0.33, 0.67])
low_threshold = price_percentiles[0.33]
high_threshold = price_percentiles[0.67]

# Use proper min value for bins (not 0)
price_min = train_df['target_price_7d'].min() - 1

print(f"   Low:    < {low_threshold:.2f} baht/kg")
print(f"   Medium: {low_threshold:.2f} - {high_threshold:.2f} baht/kg")
print(f"   High:   > {high_threshold:.2f} baht/kg")

# Categorize with proper bins
train_df['price_category'] = pd.cut(
    train_df['target_price_7d'],
    bins=[price_min, low_threshold, high_threshold, float('inf')],
    labels=['low', 'medium', 'high']
)

test_df['price_category'] = pd.cut(
    test_df['target_price_7d'],
    bins=[price_min, low_threshold, high_threshold, float('inf')],
    labels=['low', 'medium', 'high']
)

# Show distribution
print("\nüìä Category distribution:")
print("   Train:")
for cat in ['low', 'medium', 'high']:
    count = (train_df['price_category'] == cat).sum()
    pct = count / len(train_df) * 100
    print(f"      {cat:8s}: {count:8,} ({pct:5.1f}%)")
print("   Test:")
for cat in ['low', 'medium', 'high']:
    count = (test_df['price_category'] == cat).sum()
    pct = count / len(test_df) * 100
    print(f"      {cat:8s}: {count:8,} ({pct:5.1f}%)")

# ============================================================================
# Algorithm Comparison (if enabled)
# ============================================================================
if CONFIG['compare_algorithms']:
    print("\nüî¨ ALGORITHM COMPARISON - Testing on MEDIUM category")
    print("="*80)
    
    # Use medium category for comparison (usually has most data)
    train_medium = train_df[train_df['price_category'] == 'medium']
    test_medium = test_df[test_df['price_category'] == 'medium']
    
    X_train_comp = train_medium[available_features]
    y_train_comp = train_medium['target_price_7d']
    X_test_comp = test_medium[available_features]
    y_test_comp = test_medium['target_price_7d']
    
    print(f"   Comparison dataset: {len(train_medium):,} train, {len(test_medium):,} test")
    
    algorithm_comparison = {}
    
    for algo_name in CONFIG['algorithms_to_test']:
        print(f"\nü§ñ Testing {algo_name.upper()}...")
        
        try:
            # Create model
            if algo_name == 'hist_gbr':
                model = HistGradientBoostingRegressor(**ALGORITHM_PARAMS['hist_gbr'])
            elif algo_name == 'random_forest':
                model = RandomForestRegressor(**ALGORITHM_PARAMS['random_forest'])
            elif algo_name == 'xgboost':
                if not XGBOOST_AVAILABLE:
                    print(f"   ‚ö†Ô∏è  Skipping (not installed)")
                    continue
                model = xgb.XGBRegressor(**ALGORITHM_PARAMS['xgboost'])
            
            # Train
            import time
            start_time = time.time()
            model.fit(X_train_comp, y_train_comp)
            train_time = time.time() - start_time
            
            # Predict
            start_time = time.time()
            y_pred = model.predict(X_test_comp)
            predict_time = time.time() - start_time
            
            # Evaluate
            mae = mean_absolute_error(y_test_comp, y_pred)
            rmse = np.sqrt(mean_squared_error(y_test_comp, y_pred))
            r2 = r2_score(y_test_comp, y_pred)
            mape = np.mean(np.abs((y_test_comp - y_pred) / (y_test_comp + 1e-6))) * 100
            
            algorithm_comparison[algo_name] = {
                'mae': mae,
                'rmse': rmse,
                'r2': r2,
                'mape': mape,
                'train_time': train_time,
                'predict_time': predict_time
            }
            
            print(f"   ‚úÖ MAE: {mae:.2f}, RMSE: {rmse:.2f}, R¬≤: {r2:.4f}, MAPE: {mape:.2f}%")
            print(f"   ‚è±Ô∏è  Train: {train_time:.1f}s, Predict: {predict_time:.3f}s")
            
        except Exception as e:
            print(f"   ‚ùå Failed: {e}")
            continue
    
    # Select best algorithm based on MAE
    print("\n" + "="*80)
    print("üìä ALGORITHM COMPARISON RESULTS")
    print("="*80)
    print(f"\n{'Algorithm':<20} {'MAE':>8} {'RMSE':>8} {'R¬≤':>8} {'MAPE':>8} {'Train(s)':>10} {'Predict(s)':>12}")
    print("-" * 90)
    
    for algo_name, metrics in algorithm_comparison.items():
        print(f"{algo_name:<20} "
              f"{metrics['mae']:>8.2f} "
              f"{metrics['rmse']:>8.2f} "
              f"{metrics['r2']:>8.4f} "
              f"{metrics['mape']:>7.1f}% "
              f"{metrics['train_time']:>10.1f} "
              f"{metrics['predict_time']:>12.3f}")
    
    # Select best by MAE
    best_algo = min(algorithm_comparison.items(), key=lambda x: x[1]['mae'])
    best_algo_name = best_algo[0]
    best_metrics = best_algo[1]
    
    print("\n" + "="*80)
    print(f"üèÜ WINNER: {best_algo_name.upper()}")
    print("="*80)
    print(f"   MAE:  {best_metrics['mae']:.2f} baht/kg")
    print(f"   RMSE: {best_metrics['rmse']:.2f} baht/kg")
    print(f"   R¬≤:   {best_metrics['r2']:.4f}")
    print(f"   MAPE: {best_metrics['mape']:.2f}%")
    print(f"\n   This algorithm will be used for all stratified models.\n")
    
    # Set the winning algorithm
    selected_algorithm = best_algo_name
    
else:
    # Default to HistGradientBoosting if no comparison
    selected_algorithm = 'hist_gbr'
    algorithm_comparison = {}
    print("\n‚ö†Ô∏è  Algorithm comparison disabled, using HistGradientBoosting")

# ============================================================================
# Train Stratified Models with Best Algorithm
# ============================================================================
print("\nü§ñ Training Stratified Models with " + selected_algorithm.upper())
print("="*80)

models = {}
results = {}

# Get model class and params for selected algorithm
if selected_algorithm == 'hist_gbr':
    ModelClass = HistGradientBoostingRegressor
    model_params = ALGORITHM_PARAMS['hist_gbr']
elif selected_algorithm == 'random_forest':
    ModelClass = RandomForestRegressor
    model_params = ALGORITHM_PARAMS['random_forest']
elif selected_algorithm == 'xgboost':
    ModelClass = xgb.XGBRegressor
    model_params = ALGORITHM_PARAMS['xgboost']

for category in ['low', 'medium', 'high']:
    print(f"\nüîÑ Training {category.upper()} model...")
    
    train_cat = train_df[train_df['price_category'] == category]
    test_cat = test_df[test_df['price_category'] == category]
    
    if len(train_cat) < 100:
        print(f"   ‚ö†Ô∏è  Insufficient data ({len(train_cat)} samples), skipping...")
        continue
    
    X_train_cat = train_cat[available_features]
    y_train_cat = train_cat['target_price_7d']
    X_test_cat = test_cat[available_features]
    y_test_cat = test_cat['target_price_7d']
    
    print(f"   Training on {len(train_cat):,} samples...")
    model = ModelClass(**model_params)
    
    # Set verbose to 0 for cleaner output
    if hasattr(model, 'verbose'):
        model.verbose = 0
    
    model.fit(X_train_cat, y_train_cat)
    
    # Evaluate
    y_pred_test = model.predict(X_test_cat)
    test_mae = mean_absolute_error(y_test_cat, y_pred_test)
    test_rmse = np.sqrt(mean_squared_error(y_test_cat, y_pred_test))
    test_r2 = r2_score(y_test_cat, y_pred_test)
    test_mape = np.mean(np.abs((y_test_cat - y_pred_test) / (y_test_cat + 1e-6))) * 100
    
    models[category] = model
    results[category] = {
        'test_mae': test_mae,
        'test_rmse': test_rmse,
        'test_r2': test_r2,
        'test_mape': test_mape,
        'train_samples': len(train_cat),
        'test_samples': len(test_cat)
    }
    
    print(f"   ‚úÖ MAE: {test_mae:.2f}, RMSE: {test_rmse:.2f}, R¬≤: {test_r2:.4f}, MAPE: {test_mape:.2f}%")

# ============================================================================
# Train Fallback Main Model (Critical for Robustness!)
# ============================================================================
if CONFIG['train_fallback']:
    print(f"\nüîÑ Training FALLBACK main model (for robustness)...")
    print(f"   This model handles cases where category models fail")
    
    X_train_all = train_df[available_features]
    y_train_all = train_df['target_price_7d']
    X_test_all = test_df[available_features]
    y_test_all = test_df['target_price_7d']
    
    fallback_model = ModelClass(**model_params)
    if hasattr(fallback_model, 'verbose'):
        fallback_model.verbose = 0
    
    fallback_model.fit(X_train_all, y_train_all)
    
    y_pred_fallback = fallback_model.predict(X_test_all)
    fallback_mae = mean_absolute_error(y_test_all, y_pred_fallback)
    fallback_rmse = np.sqrt(mean_squared_error(y_test_all, y_pred_fallback))
    fallback_r2 = r2_score(y_test_all, y_pred_fallback)
    fallback_mape = np.mean(np.abs((y_test_all - y_pred_fallback) / (y_test_all + 1e-6))) * 100
    
    models['fallback'] = fallback_model
    results['fallback'] = {
        'test_mae': fallback_mae,
        'test_rmse': fallback_rmse,
        'test_r2': fallback_r2,
        'test_mape': fallback_mape
    }
    
    print(f"   ‚úÖ Fallback MAE: {fallback_mae:.2f}, RMSE: {fallback_rmse:.2f}, R¬≤: {fallback_r2:.4f}, MAPE: {fallback_mape:.2f}%")

# ============================================================================
# Evaluate Combined Stratified Performance
# ============================================================================
print("\nüìä Evaluating Combined Stratified Performance...")
test_df['y_pred_stratified'] = np.nan

for category in ['low', 'medium', 'high']:
    if category in models:
        mask = test_df['price_category'] == category
        X_test_cat = test_df.loc[mask, available_features]
        test_df.loc[mask, 'y_pred_stratified'] = models[category].predict(X_test_cat)

# Use fallback for any missing predictions
if CONFIG['train_fallback'] and test_df['y_pred_stratified'].isna().any():
    missing_mask = test_df['y_pred_stratified'].isna()
    X_missing = test_df.loc[missing_mask, available_features]
    test_df.loc[missing_mask, 'y_pred_stratified'] = models['fallback'].predict(X_missing)
    print(f"   ‚ÑπÔ∏è  Used fallback for {missing_mask.sum()} predictions")

test_df_eval = test_df.dropna(subset=['y_pred_stratified'])

overall_mae = mean_absolute_error(test_df_eval['target_price_7d'], test_df_eval['y_pred_stratified'])
overall_rmse = np.sqrt(mean_squared_error(test_df_eval['target_price_7d'], test_df_eval['y_pred_stratified']))
overall_r2 = r2_score(test_df_eval['target_price_7d'], test_df_eval['y_pred_stratified'])
overall_mape = np.mean(np.abs((test_df_eval['target_price_7d'] - test_df_eval['y_pred_stratified']) / (test_df_eval['target_price_7d'] + 1e-6))) * 100

print(f"\n‚úÖ Overall Stratified Performance:")
print(f"   MAE:  {overall_mae:.2f} baht/kg")
print(f"   RMSE: {overall_rmse:.2f} baht/kg")
print(f"   R¬≤:   {overall_r2:.4f}")
print(f"   MAPE: {overall_mape:.2f}%")

# ============================================================================
# Per-Crop Evaluation (Critical Insight!)
# ============================================================================
print("\nüìä Per-Crop Performance (Top 10 by volume)...")
crop_metrics = []

for crop in test_df_eval['crop_type'].value_counts().head(10).index:
    crop_data = test_df_eval[test_df_eval['crop_type'] == crop]
    if len(crop_data) < 10:
        continue
    
    crop_mae = mean_absolute_error(crop_data['target_price_7d'], crop_data['y_pred_stratified'])
    crop_r2 = r2_score(crop_data['target_price_7d'], crop_data['y_pred_stratified'])
    crop_mape = np.mean(np.abs((crop_data['target_price_7d'] - crop_data['y_pred_stratified']) / (crop_data['target_price_7d'] + 1e-6))) * 100
    avg_price = crop_data['target_price_7d'].mean()
    
    crop_metrics.append({
        'crop': crop,
        'mae': crop_mae,
        'r2': crop_r2,
        'mape': crop_mape,
        'avg_price': avg_price,
        'samples': len(crop_data)
    })
    
    print(f"   {crop:20s}: MAE={crop_mae:6.2f}, R¬≤={crop_r2:.3f}, MAPE={crop_mape:5.1f}%, Avg={avg_price:6.2f} ({len(crop_data):,} samples)")

# ============================================================================
# Save Models
# ============================================================================
print("\nüíæ Saving Models...")
os.makedirs('backend/models', exist_ok=True)

# Save stratified models
for category, model in models.items():
    if category == 'fallback':
        model_path = f'backend/models/model_c_v8_fallback.pkl'
    else:
        model_path = f'backend/models/model_c_v8_stratified_{category}.pkl'
    
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    print(f"‚úÖ Saved: {model_path}")

# Save thresholds
thresholds = {
    'low_threshold': float(low_threshold),
    'high_threshold': float(high_threshold),
    'categories': ['low', 'medium', 'high'],
    'price_min': float(price_min)
}

with open('backend/models/model_c_v8_thresholds.json', 'w') as f:
    json.dump(thresholds, f, indent=2)
print(f"‚úÖ Saved: model_c_v8_thresholds.json")

# Save features
with open('backend/models/model_c_v8_features.json', 'w') as f:
    json.dump(available_features, f, indent=2)
print(f"‚úÖ Saved: model_c_v8_features.json")

# Save comprehensive metadata
metadata = {
    'model_name': 'Model C v8.0 - Improved Stratified Price Prediction',
    'version': '8.0.0',
    'trained_at': datetime.now().isoformat(),
    'strategy': 'stratified_with_fallback',
    'algorithm': selected_algorithm,
    'algorithm_full_name': {
        'hist_gbr': 'HistGradientBoostingRegressor',
        'random_forest': 'RandomForestRegressor',
        'xgboost': 'XGBoostRegressor'
    }.get(selected_algorithm, selected_algorithm),
    'algorithm_comparison': algorithm_comparison if CONFIG['compare_algorithms'] else None,
    'improvements': [
        'Time-based split (no data leakage)',
        'Weather features included',
        'Seasonal features with cyclical encoding',
        'Algorithm comparison (3 algorithms tested)',
        'Automatic best algorithm selection',
        'Fallback model for robustness',
        'Per-crop evaluation metrics',
        'Better hyperparameters',
        'Proper bins handling'
    ],
    'config': CONFIG,
    'algorithm_params': model_params,
    'dataset': {
        'total_size': len(df_clean),
        'train_size': len(train_df),
        'test_size': len(test_df),
        'date_range': {
            'start': str(df_clean['date'].min()),
            'end': str(df_clean['date'].max()),
            'split_date': str(split_date)
        },
        'crops': int(df_clean['crop_type'].nunique()),
        'provinces': int(df_clean['province'].nunique())
    },
    'features': {
        'total': len(available_features),
        'base_price': len(base_features),
        'weather': len(weather_features),
        'seasonal': len(seasonal_features),
        'list': available_features
    },
    'price_ranges': {
        'low': f'< {low_threshold:.2f}',
        'medium': f'{low_threshold:.2f} - {high_threshold:.2f}',
        'high': f'> {high_threshold:.2f}'
    },
    'overall_performance': {
        'test_mae': float(overall_mae),
        'test_rmse': float(overall_rmse),
        'test_r2': float(overall_r2),
        'test_mape': float(overall_mape)
    },
    'stratified_performance': {
        cat: {
            'test_mae': float(results[cat]['test_mae']),
            'test_rmse': float(results[cat]['test_rmse']),
            'test_r2': float(results[cat]['test_r2']),
            'test_mape': float(results[cat].get('test_mape', 0)),
            'train_samples': int(results[cat].get('train_samples', 0)),
            'test_samples': int(results[cat].get('test_samples', 0))
        }
        for cat in ['low', 'medium', 'high'] if cat in results
    },
    'fallback_performance': {
        'test_mae': float(results['fallback']['test_mae']),
        'test_rmse': float(results['fallback']['test_rmse']),
        'test_r2': float(results['fallback']['test_r2'])
    } if 'fallback' in results else None,
    'per_crop_metrics': crop_metrics[:10]  # Top 10 crops
}

with open('backend/models/model_c_v8_metadata.json', 'w', encoding='utf-8') as f:
    json.dump(metadata, f, indent=2, ensure_ascii=False)
print(f"‚úÖ Saved: model_c_v8_metadata.json")

# ============================================================================
# Summary Report
# ============================================================================
print("\n" + "="*80)
print("‚úÖ MODEL C v8.0 TRAINING COMPLETE")
print("="*80)
print(f"\nüìä Performance Summary:")
print(f"   Overall MAE:  {overall_mae:.2f} baht/kg")
print(f"   Overall RMSE: {overall_rmse:.2f} baht/kg")
print(f"   Overall R¬≤:   {overall_r2:.4f}")
print(f"   Overall MAPE: {overall_mape:.2f}%")

if 'fallback' in results:
    print(f"\n   Fallback MAE:  {results['fallback']['test_mae']:.2f} baht/kg")
    print(f"   Fallback R¬≤:   {results['fallback']['test_r2']:.4f}")

print(f"\nüìÅ Saved Files:")
print(f"   - 3 stratified models (low, medium, high)")
print(f"   - 1 fallback model")
print(f"   - Thresholds, features, and metadata")

print(f"\nü§ñ Selected Algorithm:")
print(f"   Winner: {selected_algorithm.upper()}")
if CONFIG['compare_algorithms'] and algorithm_comparison:
    print(f"   Tested: {len(algorithm_comparison)} algorithms")
    print(f"   Best MAE: {algorithm_comparison[selected_algorithm]['mae']:.2f} baht/kg")

print(f"\nüéØ Key Improvements:")
print(f"   ‚úÖ No data leakage (time-based split)")
print(f"   ‚úÖ Weather features included")
print(f"   ‚úÖ Seasonal features with cyclical encoding")
print(f"   ‚úÖ Algorithm comparison ({len(algorithm_comparison)} tested)" if CONFIG['compare_algorithms'] else "   ‚úÖ Optimized algorithm")
print(f"   ‚úÖ Fallback model for production robustness")
print(f"   ‚úÖ Per-crop evaluation metrics")

print(f"\nüìà Comparison with v7.0:")
v7_mae = 6.97
v7_r2 = 0.7589
mae_improvement = ((v7_mae - overall_mae) / v7_mae) * 100
r2_improvement = ((overall_r2 - v7_r2) / v7_r2) * 100

print(f"   MAE:  {v7_mae:.2f} ‚Üí {overall_mae:.2f} ({mae_improvement:+.1f}%)")
print(f"   R¬≤:   {v7_r2:.4f} ‚Üí {overall_r2:.4f} ({r2_improvement:+.1f}%)")

print("\n" + "="*80)
print("üöÄ Ready to deploy! Update model_c_wrapper.py to use v8 models.")
print("="*80)
